
<!-- 2.2 Functional Requirements – ETL of Data -->
<!-- 2.2.3 Workflow Automation with Oozie -->
<!-- Create a Oozie workflow that will automate Sqoop data Loading and Hive table creation -->


<!-- 1) Create an Oozie Workflow that will automate the processes of steps 2.2.1 and 2.2.2. -->
<!--  Each of the files in step 2.2.1 should be deleted before the workflow is executed in order to prevent storage of redundant data -->
<!--  The tables created in step 2.2.2 should be dropped before executing the hive workflow in order to prevent redundancy. -->

<!-- 2) Incorporate that workflow into an Oozie Coordinator
that will execute with the following conditions:
 Every weekday between 08:00 and 18:00 EST
 Executes once every 20 minutes
 Starts on September 6th 2018 at 08:00 EST
 Ends on March 29th 2025 at 18:00 EST -->





<!-- INSTRUCTIONS -->
<!-- create a "credit_card_system_workflow" directory under user maria_dev and upload all hive scripts (hql files) as well as this workflow.xml to it -->
<!-- make sure to place the job.properties file in the Documents folder of your Linux VM -->



<!-- <?xml version="1.0" encoding="UTF-8"?> -->

<workflow-app xmlns = "uri:oozie:workflow:0.2" name = "credit_card_system_workflow">
    <start to="sqoop-branch-node"/>  




<!-- Sqoop Nodes (import data from RDBMS)-->



    <action name="sqoop-branch-node">    <!--node name--> 
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>  <!--job tracking node is http://sandbox.hortonworks.com:8050 which is define in job properties--> 
            <name-node>${nameNode}</name-node> <!--name node is hdfs://sandbox.hortonworks.com:8020 which is defined in job properties--> 
            <prepare>
                <delete path="${nameNode}/user/maria_dev/Credit_Card_System"/>
            </prepare>
 
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value> <!-- queue name is default and is defined in job properties -->
                </property>
            </configuration>
            <command>job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop --exec branch_import</command>
            <!-- get data from MySQL RDBMS using Sqoop and store data at /user/maria_dev/Credit_Card_System --> 
            <!-- you may have to create a seperate sqoop action node for each of these :(( -->
        </sqoop>
        <ok to="sqoop-cc-node"/>    <!-- if everything is ok then start Hive nodes --> 
        <error to="sqoop-fail"/>
    </action>


    <action name="sqoop-cc-node">  
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker> 
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
            </configuration>
            <command>job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop --exec credit_card_import</command>
        </sqoop>
        <ok to="sqoop-time-node"/>
        <error to="sqoop-fail"/>
    </action>


    <action name="sqoop-time-node">  
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker> 
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
            </configuration>
            <command>job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop --exec time_import</command>
        </sqoop>
        <ok to="sqoop-customer-node"/>
        <error to="sqoop-fail"/>
    </action>


    <action name="sqoop-customer-node">  
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker> 
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
            </configuration>
            <command>job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop --exec customer_import</command>
        </sqoop>
        <ok to="hive-branch-node"/>
        <error to="sqoop-fail"/>
    </action>
    







<!-- Hive Nodes (create staging and data warehouse tables, insert data into tables) -->







   
    <action name="hive-branch-node">
        <hive xmlns="uri:oozie:hive-action:0.4">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <script>${nameNode}/user/maria_dev/credit_card_system_workflow/branch.hql</script>
        </hive>
        <ok to="hive-cc-node"/>
        <error to="hive-fail"/>
    </action>


   
    <action name="hive-cc-node">
        <hive xmlns="uri:oozie:hive-action:0.4">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <script>${nameNode}/user/maria_dev/credit_card_system_workflow/credit_card.hql</script>
        </hive>
        <ok to="hive-customer-node"/>
        <error to="hive-fail"/>
    </action>

  
      
    <action name="hive-customer-node">
        <hive xmlns="uri:oozie:hive-action:0.4">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <script>${nameNode}/user/maria_dev/credit_card_system_workflow/customer.hql</script>
        </hive>
        <ok to="hive-time-node"/>
        <error to="hive-fail"/>
    </action>
   


    <action name="hive-time-node">
        <hive xmlns="uri:oozie:hive-action:0.4">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <script>${nameNode}/user/maria_dev/credit_card_system_workflow/time.hql</script>
        </hive>
        <ok to="end" />
        <error to="hive-fail" />
    </action>





    <kill name="sqoop-fail"><message>Sqoop Job failed - error message[${wf:errorMessage(wf:lastErrorNode())}]</message></kill>
    <kill name="hive-fail"><message>Hive Script failed - error message[${wf:errorMessage(wf:lastErrorNode())}]</message></kill>	

    <end name="end" />
</workflow-app>



<!-- LOGIN TO PUTTY OR SANDBOX CLI AS 'root' -->
<!-- RUN THIS JOB  USING THE FOLLOWING COMMANDS => make sure to place the job.properties file in the Linux VM Documents folder  -->
<!-- [root@sandbox ~]$ cd Documents  -->
<!-- [root@sandbox Documents]$ oozie job   -oozie  http://localhost:11000/oozie  -config  job.properties  -run -->


<!-- MONITOR OOZIE JOB FROM OOZIE WEB CONSOLE -->
<!-- => navigate to your IP address :11000/oozie/        // 192.168.109.129:11000/oozie/ -->









